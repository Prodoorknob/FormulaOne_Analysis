# -*- coding: utf-8 -*-
"""formula1-data-analysis-doorknob (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dy0Ct73xRXR3_gwiY7ymn8yWn7r4j9pK
"""

import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import re
import datetime
import seaborn as sns
from sklearn.feature_selection import RFECV
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPClassifier
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import precision_score
from sklearn.svm import SVC
from sklearn.svm import SVR
from sklearn import svm
from sklearn.metrics import classification_report, confusion_matrix  
from sklearn.model_selection import GridSearchCV

from google.colab import drive
drive.mount('/content/drive')

path='/content/drive/MyDrive/Colab Notebooks/F1/F1/'
constructors_raw=pd.read_csv(path+"constructors.csv")
constructors_st_raw=pd.read_csv(path+"constructor_standings.csv")
constructors_res_raw=pd.read_csv(path+"constructor_results.csv")
drivers_raw=pd.read_csv(path+"drivers.csv")
driver_st_raw=pd.read_csv(path+"driver_standings.csv")
results_raw=pd.read_csv(path+"results.csv")
status=pd.read_csv(path+"status.csv")
races_raw=pd.read_csv(path+"races.csv")
pit_stops_raw=pd.read_csv(path+"pit_stops.csv")
quali_raw=pd.read_csv(path+"qualifying.csv")
lap_times_raw=pd.read_csv(path+"lap_times.csv")

races_raw=races_raw[races_raw["raceId"]>=841]
races_raw=races_raw.sort_values(["raceId"],ascending=True)
results_raw=results_raw[results_raw["raceId"]>=841]
results_raw=results_raw.sort_values(["raceId"],ascending=True)
races_raw_int=results_raw.merge(status,how='outer')
races_raw_int.head()

results=races_raw.merge(races_raw_int,how='outer')
results.columns

results_drop=['url','resultId','positionText','statusId']
results=results.drop(columns=results_drop,axis=1)
results.head()

col=["driverRef","number","code","url"]
drivers_raw=drivers_raw.drop(col,axis=1)
drivers_raw['dob']=pd.to_datetime(drivers_raw['dob'])
drivers_raw['Age']=2021-drivers_raw['dob'].dt.year
drivers_raw=drivers_raw.drop("dob",axis=1)
drivers_raw.shape

driver_st_raw=driver_st_raw[driver_st_raw["raceId"]>=841]
driver_st_raw=driver_st_raw.sort_values(["raceId","driverId"],ascending=True)
drop_columns=['positionText','driverStandingsId']
driver_st_raw=driver_st_raw.drop(columns=drop_columns,axis=1)

driver_st_raw.head()

driver_st_raw=driver_st_raw.rename({'points':'driver_points','position':'driver_position','wins':'driver_wins'},axis=1)
driver_data=drivers_raw.merge(driver_st_raw,how='outer')
driver_data.head(10)
#driver_data.shape

constructors_st_raw=constructors_st_raw[constructors_st_raw["raceId"]>=841]
constructors_res_raw=constructors_res_raw[constructors_res_raw["raceId"]>=841]
constructors_st_raw=constructors_st_raw.sort_values(["raceId"],ascending=True)
constructors_res_raw=constructors_res_raw.sort_values(["raceId"],ascending=True)
constructors_raw=constructors_raw.rename({'name':'Cons_name','nationality':'Cons_nationality','points':'Cons_points','position':'Cons_position','wins':'Cons_wins'},axis=1)
constructors_st_raw=constructors_st_raw.rename({'name':'Cons_name','nationality':'Cons_nationality','points':'Cons_points','position':'Cons_position','wins':'Cons_wins'},axis=1)
constructors_raw=constructors_raw.drop(columns=['constructorRef','url'],axis=1)

constructors_data=constructors_raw.merge(constructors_st_raw,how='outer')
c_d=["constructorStandingsId","positionText"]
constructors_data=constructors_data.drop(columns=c_d,axis=1)
print(constructors_data.head(10))

pit_stops_raw=pit_stops_raw.drop(["time","duration"],axis=1)
pit_stops_raw=pit_stops_raw.sort_values(["raceId","driverId"],ascending=True)
quali_raw=quali_raw.drop(["qualifyId","number"],axis=1)

"""# **Start of Lap_Times dataset Cleaning**"""

new_lap_times=pd.DataFrame(lap_times_raw.groupby(["raceId","driverId"]).mean(["milliseconds"]).reset_index())

rename1={'milliseconds':'Avg_lap_time','lap':'total_laps'}
new_lap_times=new_lap_times.rename(columns=rename1)
new_lap_times=new_lap_times.drop(['position'],axis=1)

race_index=new_lap_times.loc[new_lap_times["raceId"]==841].index.min()
new_lap_times=new_lap_times[race_index:]

print(new_lap_times.head())
new_lap_times.shape

"""# **End of Lap_Times dataset cleaning**

# **Start of Quali_data cleaning**
"""

pit_stops_raw=pit_stops_raw.rename(columns={"lap":"laps_pit","milliseconds":"avg_pit_ms"})
quali_raw=quali_raw.rename(columns={"position":"q_pos"})
q_race_index=quali_raw.loc[quali_raw["raceId"]==841].index.min()
q_race_index

quali_raw=quali_raw[q_race_index:]

quali_raw=quali_raw.sort_values(["raceId","driverId"],ascending=True)
quali_raw=quali_raw.drop(7276,axis=0)

# quali_raw['best_quali_time']=0
time_convert=['q1','q2','q3']
for t in time_convert:
    #quali_raw[t]=re.sub(r"\\N",'',quali_raw[t])
    #print(quali_raw['q1'].head())
    quali_raw[t]=quali_raw[t].str.replace(r'\\N','')
    #print(quali_raw.head(15))
quali_raw=quali_raw.replace(r'^\s*$',np.nan,regex=True)
#quali_raw.isnull().sum()
quali_raw.head(15)

"""Modify Quali excel to have best quali time over three sessions
--convert to milliseconds
--find the lowest time

# **End of Quali_data cleaning**

# **Start of Pit_Stops_Cleaning**
"""

pit_stops_raw.head(10)

pit_stops_raw=pit_stops_raw.groupby(["raceId","driverId"]).agg({'stop':'count','laps_pit': list , 'avg_pit_ms':'mean'}).reset_index()
pit_filtered=pit_stops_raw.drop(columns="laps_pit",axis=1)

pit_filtered.head(10)

"""# **End of Pit_stops cleaning**"""

print(quali_raw.head())
print(pit_filtered.head())
print(new_lap_times.head())

race_data=new_lap_times.merge(pit_filtered,how='outer')
print(race_data.head())
print(race_data.shape)

#results.head()

weekend_data=race_data.merge(quali_raw,how='inner')
#print(weekend_data.head(30))
# print(weekend_data.shape)
# driver_data.head()
weekend_data['stop']=weekend_data['stop'].fillna(int(weekend_data['stop'].mean()))
weekend_data['avg_pit_ms']=weekend_data['avg_pit_ms'].fillna(weekend_data['avg_pit_ms'].mean())
q=['q1','q2','q3']
for a in q:
    weekend_data[a]=weekend_data[a].str.replace('.',':')
    weekend_data[a]=weekend_data[a].str.split(':')
    weekend_data[a]=weekend_data[a].fillna(0)
    
    for index,t in weekend_data.iterrows():
        if t[a]==0:
            pass
        else:
            t_ms=(int(t[a][0])*60*1000)+(int(t[a][1])*1000)+int(t[a][2])
            weekend_data.loc[(weekend_data['raceId']==t['raceId']) & (weekend_data['driverId']==t['driverId']),a]=t_ms
#             weekend_data.loc[a]=weekend_data[a].replace(t,t_ms)
#     print(t_sec) 
weekend_data.isnull().sum()
weekend_data.head(20)

weekend_data['fastest_time']=0
for index,row in weekend_data.iterrows():
    #print(row['q1'])
    if (row['q1']!=0) & (row['q2']==0 & row['q3']==0):
        time=row['q1']
        weekend_data.loc[(weekend_data['raceId']==row['raceId']) & (weekend_data['driverId']==row['driverId']),'fastest_time']=time
    elif(row['q1']!=0 & row['q2']!=0) & (row['q3']==0):
        time=min(row['q1'],row['q2'])
        weekend_data.loc[(weekend_data['raceId']==row['raceId']) & (weekend_data['driverId']==row['driverId']),'fastest_time']=time
    else:
        time=min(row['q1'],row['q2'],row['q3'])
        weekend_data.loc[(weekend_data['raceId']==row['raceId']) & (weekend_data['driverId']==row['driverId']),'fastest_time']=time
print(weekend_data['fastest_time'])

overall_data_a=results.merge(weekend_data,how='inner')
overall_data_b=overall_data_a.merge(driver_data,how='inner')
overall_data_c=overall_data_b.merge(constructors_data,how='inner')

overall_data_c.columns

overall_data_c['Driver_name']=overall_data_c['forename']+ ' ' +overall_data_c['surname']

overall_drop=['circuitId','race_time','total_laps','forename','surname']
overall_data=overall_data_c.drop(columns=overall_drop,axis=1)
columns_order=['raceId','year','round','name','date','driverId','Driver_name','number','nationality','Age','driver_points','driver_wins','driver_position','constructorId','Cons_name','Cons_nationality','Cons_points','Cons_wins','Cons_position','q_pos','fastest_time','q1','q2','q3','grid','positionOrder','position','points','laps','time','milliseconds','fastestLap','fastestLapTime','fastestLapSpeed','Avg_lap_time','stop','avg_pit_ms','rank','status']
overall_data=overall_data[columns_order]
drop_data=['raceId','date','driverId','number','Cons_nationality','constructorId','Cons_points','Cons_position','Cons_wins','q_pos','fastestLapTime','driver_points','driver_wins','driver_position','q1','q2','q3','points','laps','time','milliseconds','fastestLap','fastestLapSpeed','rank','status','position']
final_data=overall_data.drop(drop_data,axis=1)
final_data.head()

df_dum = pd.get_dummies(final_data, columns = ['name', 'nationality', 'Cons_name'] )
processed_data=final_data.merge(df_dum, how='inner')
processed_data=processed_data.drop(columns = ['name', 'nationality', 'Cons_name'], axis=1)

def race_info(race,driver):
    race_data=overall_data[(overall_data['name']==race) & (overall_data['Driver_name']==driver)]
    return race_data
#print(race_select('Hungarian Grand Prix','Sebastian Vettel'))

data_corr=final_data.corr()
mask = np.triu(np.ones_like(data_corr, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(data_corr, mask=mask, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={"shrink": .5})

processed_data.columns

processed_data['year'].describe()

def score_regression(model):
    score = 0
    for circuit in df[df.year == 2020]['round'].unique():

        test = df[(df.year == 2020) & (df['round'] == circuit)]
        X_test = test.drop(['Driver_name', 'positionOrder'], axis = 1)
        y_test = test.positionOrder

        #scaling
        X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)

        # make predictions
        prediction_df = pd.DataFrame(model.predict(X_test), columns = ['results'])
        prediction_df['podium'] = y_test.reset_index(drop = True)
        prediction_df['actual'] = prediction_df.podium.map(lambda x: 1 if x == 1 else 0)
        prediction_df.sort_values('results', ascending = True, inplace = True)
        prediction_df.reset_index(inplace = True, drop = True)
        prediction_df['predicted'] = prediction_df.index
        prediction_df['predicted'] = prediction_df.predicted.map(lambda x: 1 if x == 0 else 0)

        score += precision_score(prediction_df.actual, prediction_df.predicted)

    model_score = score / df[df.year == 2020]['round'].unique().max()
    return model_score, prediction_df

def score_classification(model):
    score = 0
    for circuit in df[df.year == 2020]['round'].unique():

        test = df[(df.year == 2020) & (df['round'] == circuit)]
        X_test = test.drop(['Driver_name', 'positionOrder'], axis = 1)
        y_test = test.positionOrder

        #scaling
        X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)

        # make predictions
        prediction_df = pd.DataFrame(model.predict_proba(X_test), columns = ['proba_0', 'proba_1'])
        prediction_df['actual'] = y_test.reset_index(drop = True)
        prediction_df.sort_values('proba_1', ascending = False, inplace = True)
        prediction_df.reset_index(inplace = True, drop = True)
        prediction_df['predicted'] = prediction_df.index
        prediction_df['predicted'] = prediction_df.predicted.map(lambda x: 1 if x == 0 else 0)

        score += precision_score(prediction_df.actual, prediction_df.predicted)

    model_score = score / df[df.year == 2020]['round'].unique().max()
    return model_score, prediction_df

df = processed_data.copy()
df.positionOrder = df.positionOrder.map(lambda x: 1 if x == 1.0 else 0)
train = df[df['year']< 2020]
x_train = train.drop(['Driver_name', 'positionOrder'], axis = 1)
y_train = train.positionOrder
scaler = StandardScaler()
x_train = pd.DataFrame(scaler.fit_transform(x_train), columns = x_train.columns)
model_log=LogisticRegression(penalty='l1',C=0.21544346900318834,solver='saga')
model_ran=RandomForestClassifier(criterion='gini',max_depth=9.0,max_features='auto')
model_svc=svm.SVC(C=2.154434690031882,gamma= 0.016681005372000592, kernel= 'sigmoid',probability=True,random_state=3)
model_nn=MLPClassifier(max_iter=800, activation='relu',alpha=2.154434690031882,hidden_layer_sizes=(75, 25, 50, 10), solver='sgd')
model_nn.fit(x_train,y_train)
model_svc.fit(x_train,y_train)
model_ran.fit(x_train,y_train)
model_log.fit(x_train,y_train)
Logistic_score, Logistic_pred=score_classification(model_log)
RandomForest_score, RandomForest_pred=score_classification(model_ran)
svc_score, svc_pred=score_classification(model_svc)
nn_score, nn_pred=score_classification(model_nn)

Logistic_pred.head()
Logistic_score
RandomForest_score
RandomForest_pred.head()

nn_params_reg={'activation': 'tanh', 'alpha': 10.0, 'hidden_layer_sizes': (80, 20, 40, 5), 'solver': 'adam'}
df = processed_data.copy()
df=df.reset_index()
train = df[df.year <2020]
x_train = train.drop(['Driver_name', 'positionOrder'], axis = 1)
y_train = train.positionOrder
scaler = StandardScaler()
x_train = pd.DataFrame(scaler.fit_transform(x_train), columns = x_train.columns)
# model_log=LogisticRegression(penalty='l1',C='0.21544346900318834',solver='saga')
# model_ran=RandomForestClassifier(criterion='gini',max_depth='9.0',max_features='auto')
model_lr= LinearRegression()
model_svr=svm.SVR(C=4.64,gamma= 0.004641, kernel= 'rbf')
model_nn=MLPRegressor(max_iter=800, activation='tanh',alpha=10,hidden_layer_sizes=(80, 20, 40, 5), solver='adam')
model_nn.fit(x_train,y_train)
model_svr.fit(x_train,y_train)
model_lr.fit(x_train,y_train)
svr_score, svr_score_reg=score_regression(model_svr)
nn_score_r, nn_score_reg=score_regression(model_nn)
lr_score,lr_reg=score_regression(model_lr)

svr_score
nn_score_r
lr_score

# df = processed_data.copy()
# df=df.reset_index()
# #train split
# train = df[df.year <2020]
# X_train = train.drop(['Driver_name', 'positionOrder'], axis = 1)
# y_train = train.positionOrder

# scaler = StandardScaler()
# X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)
# #linear regression
# model= LinearRegression()
# model.fit(X_train, y_train)
# LR_score=score_regression(model)
# # Support Vector Machines
# # params={'gamma': np.logspace(-4, -1, 10),
# #         'C': np.logspace(-2, 1, 10),
# #         'kernel': ['linear', 'poly', 'rbf', 'sigmoid']} 
# # model=svm.SVR()
# # GSCV=GridSearchCV(model,params,cv=3,verbose=2)
# # GSCV.fit(X_train, y_train)
# # print(GSCV.best_params_)
# # SVC_score=score_regression(GSCV)
# # Neural network
# params_nn={'hidden_layer_sizes': [(80,20,40,5), (75,30,50,10,3)], 
#         'activation': [ 'relu','logistic', 'tanh'], 
#         'solver': ['sgd', 'adam'], 
#         'alpha': np.logspace(-2,1,10)} 

# NN_model= MLPRegressor(max_iter=800)
# GSCV_nn=GridSearchCV(NN_model,params_nn,cv=3,verbose=2)
# GSCV_nn.fit(X_train, y_train)
# print(GSCV_nn.best_params_)
# NN_score=score_regression(GSCV_nn)

# df = processed_data.copy()
# df.positionOrder = df.positionOrder.map(lambda x: 1 if x == 1.0 else 0)

# #split train

# train = df[df.year <2020]
# X_train = train.drop(['Driver_name', 'positionOrder'], axis = 1)
# y_train = train.positionOrder

# scaler = StandardScaler()
# X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)


# # Logistic Regression

# params_lr={'penalty': ['l1', 'l2'],
#         'solver': ['saga', 'liblinear'],
#         'C': np.logspace(-2,1,10)}
# model=LogisticRegression()
# GSCV=GridSearchCV(model,params_lr,cv=3,verbose=2)
# GSCV.fit(X_train, y_train)
# print(GSCV.best_params_)
# Log_score=score_classification(GSCV)

# # Random Forest Classifier

# params={'criterion': ['gini', 'entropy'],
#         'max_features': [0.8, 'auto', None],
#         'max_depth': list(np.linspace(5, 55, 26)) + [None]}
# model_ran=RandomForestClassifier()
# GSCV_RFC=GridSearchCV(model_ran,params,cv=3,verbose=2)
# GSCV_RFC.fit(X_train, y_train)
# print(GSCV_RFC.best_params_)
# RFC_score=score_classification(GSCV_RFC)
# # Support Vector Machines

# params={'gamma': np.logspace(-2, -1, 10),
#         'C': np.logspace(-2, 1, 10),
#         'kernel': ['linear', 'poly', 'rbf', 'sigmoid']} 
# model_svc = svm.SVC()
# GSCV_SVC=GridSearchCV(model_svc,params,cv=3,verbose=1)
# GSCV_SVC.fit(X_train, y_train)
# print(GSCV_SVC.best_params_)
# #SVC_score=score_classification(GSCV_SVC)

# # Neural network

# params_nn={'hidden_layer_sizes': [(80,20,40,5), (75,25,50,10)], 
#         'activation': [ 'logistic', 'tanh', 'relu'], 
#         'solver': ['sgd', 'adam'], 
#         'alpha': np.logspace(-2,1,10)} 
# NN_model= MLPClassifier(max_iter=800)
# GSCV_nn=GridSearchCV(NN_model,params_nn,cv=3,verbose=2)
# GSCV_nn.fit(X_train, y_train)
# print(GSCV_nn.best_params_)
# NN_score=score_regression(GSCV_nn)

# nn_params_class={'activation': 'relu', 'alpha': 2.154434690031882, 'hidden_layer_sizes': (75, 25, 50, 10), 'solver': 'sgd'}
# svc_params_class={'C': 2.154434690031882, 'gamma': 0.016681005372000592, 'kernel': 'sigmoid'}
# rf_params_class={'criterion': 'gini', 'max_depth': 9.0, 'max_features': 'auto'}
# lr_params_class={'C': 0.21544346900318834, 'penalty': 'l1', 'solver': 'saga'}

# df = processed_data.copy()
# df.positionOrder = df.positionOrder.map(lambda x: 1 if x == 1.0 else 0)

# #split train

# train = df[df.year <2020]
# X_train = train.drop(['Driver_name', 'positionOrder'], axis = 1)
# y_train = train.positionOrder

# scaler = StandardScaler()
# X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)


# # gridsearch dictionary

# comparison_dict=defaultdict(list)

# # Logistic Regression

# params={'penalty': ['l1', 'l2'],
#         'solver': ['saga', 'liblinear'],
#         'C': np.logspace(-3,1,20)}
# # comparison_dict=defaultdict(list)
# for penalty in params['penalty']:
#     for solver in params['solver']:
#         for c in params['C']:
#             model_params = (penalty, solver, c)
#             model = LogisticRegression(penalty = penalty, solver = solver, C = c, max_iter = 10000)
#             model.fit(X_train, y_train)
            
#             model_score = score_classification(model)
            
#             comparison_dict['model'].append('logistic_regression')
#             comparison_dict['params'].append(model_params)
#             comparison_dict['score'].append(model_score)

# # Random Forest Classifier

# params={'criterion': ['gini', 'entropy'],
#         'max_features': [0.8, 'auto', None],
#         'max_depth': list(np.linspace(5, 55, 26)) + [None]}

# for criterion in params['criterion']:
#     for max_features in params['max_features']:
#         for max_depth in params['max_depth']:
#             model_params = (criterion, max_features, max_depth)
#             model = RandomForestClassifier(criterion = criterion, max_features = max_features, max_depth = max_depth)
#             model.fit(X_train, y_train)
            
#             model_score = score_classification(model)
            
#             comparison_dict['model'].append('random_forest_classifier')
#             comparison_dict['params'].append(model_params)
#             comparison_dict['score'].append(model_score)

# # Support Vector Machines

# params={'gamma': np.logspace(-4, -1, 20),
#         'C': np.logspace(-2, 1, 20),
#         'kernel': ['linear', 'poly', 'rbf', 'sigmoid']} 

# for gamma in params['gamma']:
#     for c in params['C']:
#         for kernel in params['kernel']:
#             model_params = (gamma, c, kernel)
#             model = svm.SVC(probability = True, gamma = gamma, C = c, kernel = kernel )
#             model.fit(X_train, y_train)
            
#             model_score = score_classification(model)
            
#             comparison_dict['model'].append('svm_classifier')
#             comparison_dict['params'].append(model_params)
#             comparison_dict['score'].append(model_score)

# # Neural network

# params={'hidden_layer_sizes': [(80,20,40,5), (75,25,50,10)], 
#         'activation': ['identity', 'logistic', 'tanh', 'relu'], 
#         'solver': ['lbfgs', 'sgd', 'adam'], 
#         'alpha': np.logspace(-4,2,20)} 

# for hidden_layer_sizes in params['hidden_layer_sizes']:
#     for activation in params['activation']:
#         for solver in params['solver']:
#             for alpha in params['alpha']:
#                 model_params = (hidden_layer_sizes, activation, solver, alpha )
#                 model = MLPClassifier(hidden_layer_sizes = hidden_layer_sizes,activation = activation, solver = solver, alpha = alpha, random_state = 1)
#                 model.fit(X_train, y_train)

#                 model_score = score_classification(model)

#                 comparison_dict['model'].append('neural_network_classifier')
#                 comparison_dict['params'].append(model_params)
#                 comparison_dict['score'].append(model_score)

# df = processed_data.copy()

# #train split

# train = df[df.year <2020]
# X_train = train.drop(['Driver_name', 'positionOrder'], axis = 1)
# y_train = train.positionOrder

# scaler = StandardScaler()
# X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)

  
# # Linear Regression

# params={'fit_intercept': ['True', 'False']}
# comparison_dict=defaultdict(list)
# for fit_intercept in params['fit_intercept']:
#     model_params = (fit_intercept)
#     model = LinearRegression(fit_intercept = fit_intercept)
#     model.fit(X_train, y_train)
            
#     model_score = score_regression(model)
            
#     comparison_dict['model'].append('linear_regression')
#     comparison_dict['params'].append(model_params)
#     comparison_dict['score'].append(model_score)

    
# # Random Forest Regressor

# params={'criterion': ['mse'],
#         'max_features': [0.8, 'auto', None],
#         'max_depth': list(np.linspace(5, 55, 26)) + [None]}

# for criterion in params['criterion']:
#     for max_features in params['max_features']:
#         for max_depth in params['max_depth']:
#             model_params = (criterion, max_features, max_depth)
#             model = RandomForestRegressor(criterion = criterion,
#                                           max_features = max_features, max_depth = max_depth, random_state = 1)
#             model.fit(X_train, y_train)
            
#             model_score = score_regression(model)
            
#             comparison_dict['model'].append('random_forest_regressor')
#             comparison_dict['params'].append(model_params)
#             comparison_dict['score'].append(model_score)

            
# # Support Vector Machines

# params={'gamma': np.logspace(-4, -1, 10),
#         'C': np.logspace(-2, 1, 10),
#         'kernel': ['linear', 'poly', 'rbf', 'sigmoid']} 

# for gamma in params['gamma']:
#     for c in params['C']:
#         for kernel in params['kernel']:
#             model_params = (gamma, c, kernel)
#             model = svm.SVR(gamma = gamma, C = c, kernel = kernel)
#             model.fit(X_train, y_train)
            
#             model_score = score_regression(model)
            
#             comparison_dict['model'].append('svm_regressor')
#             comparison_dict['params'].append(model_params)
#             comparison_dict['score'].append(model_score)

            
# # Neural network

# params={'hidden_layer_sizes': [(80,20,40,5), (75,30,50,10,3)], 
#         'activation': ['identity', 'relu','logistic', 'tanh',], 
#         'solver': ['lbfgs','sgd', 'adam'], 
#         'alpha': np.logspace(-4,1,20)} 

# for hidden_layer_sizes in params['hidden_layer_sizes']:
#     for activation in params['activation']:
#         for solver in params['solver']:
#             for alpha in params['alpha']:
#                 model_params = (hidden_layer_sizes, activation, solver, alpha )
#                 model = MLPRegressor(hidden_layer_sizes = hidden_layer_sizes,
#                                       activation = activation, solver = solver, alpha = alpha, random_state = 1)
#                 model.fit(X_train, y_train)

#                 model_score = score_regression(model)

#                 comparison_dict['model'].append('nn_regressor')
#                 comparison_dict['params'].append(model_params)
#                 comparison_dict['score'].append(model_score)
# pd.DataFrame(comparison_dict).groupby('model')['score'].max()